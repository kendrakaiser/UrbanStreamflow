#add sreamflow to current reservoir storage
res[i+1,5]=res[i,5]+q_in[i]-q_out[i,1]
res[i+1,6]=res[i,6]+q_in[i]-q_out[i,2]
res[i+1,7]=res[i,7]+q_in[i]-q_out[i,3]
res[i+1,8]=res[i,8]+q_in[i]-q_out[i,4]
}
resL<-data.frame(res[,])
print(resL)
}
resOut(int, q_in)
q_out[i,1] = res[i,5] + q_in[i] - (rc[i+1]*1.05)
i=1
#rules for less risk averse manager (always +5% of rule curve)
if (res[i,5] >= (rc[i+1]*1.05)){
q_out[i,1] = res[i,5] + q_in[i] - (rc[i+1]*1.05)
} else if (res[i,5] < rc[i+1]){q_out[i,1] = 0
} else{q_out[i,1]=0}
View(q_out)
View(res)
shiny::runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
runApp('Dropbox/BSU/R/WaterActors/ABM')
library(shiny)
runApp('Dropbox/BSU/R/WaterActors/Tester/tester')
library(devtools)
library(dataRetrieval)
#plug in site number or parameter code and start/end date
siteNo <- "07374000"
pCode <- c("00060" ,"00065")
start.date <- "2004-03-17"
end.date <- "2017-12-31"
miss <-readNWISuv(siteNumbers = siteNo, parameterCd = pCode,startDate = start.date,endDate = end.date)
names(miss)
View(miss)
colnames(miss)<- c("ag", "site", "dateTime","flow", "flcd", "stage","stagecd", "tz")
View(miss)
flOrd<-order(miss$flow)
rank<- 1:419749
hist(flOrd)
flOrd<-data.frame(order(miss$flow), rank)
View(flOrd)
plot(flOrd$rank, flOrd$order.miss.flow.)
flow<-order(miss$flow)
max(flow)
min(flow)
flow<-as.numeric(order(miss$flow))
flow<-data.frame(as.numeric(order(miss$flow)))
View(flow)
View(flow)
flow<-sort(miss$flow)
flow<-data.frame(sort(miss$flow))
View(flow)
flow.uq<-unique(flow)
rank<- 1:912
flow.ix<-order(miss$flow)
View(flow.uq)
flow.rank<-data.frame(flow.uq, rank)
View(flow.rank)
flow2<-flow[flow$sort.miss.flow. == flow.rank$flow.uq, flow.rank$rank]
flow2<-flow[flow$sort.miss.flow. == flow.rank$flow.uq,]
flow2
flow2<- merge(flow, flow.rank, by="srt.miss.flow.")
flow2<- merge(flow, flow.rank, by="sort.miss.flow.")
View(flow2)
flow2$recInt<- rank/418885
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$sort.miss.flow.), unique(flow2$recInt))
flow<-data.frame(sort(miss$flow), decreasing=FALSE)
flow.uq<-unique(flow)
flow.rank<-data.frame(flow.uq, rank)
flow2<- merge(flow, flow.rank, by="sort.miss.flow.")
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$sort.miss.flow.), unique(flow2$recInt))
flow<-data.frame(sort(miss$flow), decreasing=TRUE)
flow.uq<-unique(flow)
rank<- 1:912
flow.rank<-data.frame(flow.uq, rank)
flow2<- merge(flow, flow.rank, by="sort.miss.flow.")
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$sort.miss.flow.), unique(flow2$recInt))
flow<-data.frame(sort(miss$flow, decreasing=TRUE))
#flow.ix<-order(miss$flow) #index of low to high
flow.uq<-unique(flow)
rank<- 1:912
flow.rank<-data.frame(flow.uq, rank)
flow2<- merge(flow, flow.rank, by="sort.miss.flow.")
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$sort.miss.flow.), unique(flow2$recInt))
flow<-data.frame(sort(miss$flow, decreasing=FALSE))
#flow.ix<-order(miss$flow) #index of low to high
flow.uq<-unique(flow)
rank<- 1:912
flow.rank<-data.frame(flow.uq, rank)
flow2<- merge(flow, flow.rank, by="sort.miss.flow.")
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$sort.miss.flow.), unique(flow2$recInt))
colnames(flow[1]) <-c("miss.flow")
colnames(flow)
colnames(flow$sort.miss.flow..decreasing...FALSE.) <-"miss.flow"
colnames(flow) <-"miss.flow"
colnames(flow.rank) <-c("miss.flow", "rank")
flow2<- merge(flow, flow.rank, by="miss.flow")
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$sort.miss.flow.), unique(flow2$recInt))
plot(unique(flow2$miss.flow), unique(flow2$recInt))
flow<-data.frame(sort(miss$flow, decreasing=TRUE))
#flow.ix<-order(miss$flow) #index of low to high
flow.uq<-unique(flow)
rank<- 1:912
flow.rank<-data.frame(flow.uq, rank)
colnames(flow) <-"miss.flow"
colnames(flow.rank) <-c("miss.flow", "rank")
flow2<- merge(flow, flow.rank, by="miss.flow")
flow2$recInt<- flow2$rank/418885
plot(unique(flow2$miss.flow), unique(flow2$recInt))
plot(unique(flow2$miss.flow), unique(flow2$recInt), typ="l")
flow2$exd<- flow2$rank/418885
flow2$recInt<-1/flow2$exd
#exceedence probability
plot(unique(flow2$miss.flow), unique(flow2$recInt), typ="l")
plot(miss.stage, miss.flow, type='l')
plot(miss$stage, miss$flow, type='l')
plot(miss$stage, miss$flow)
flow2$recInt[flow2$miss.flow == 747000]
2880 * 1405
unique(flow2$recInt[flow2$miss.stage == 30])
flow2$recInt[flow2$miss.stage == 30]
unique(flow2$recInt[flow2$stage == 30])
1405/2880
unique(miss$flow[miss$stage ==35])
miss$flow[miss$stage ==35]
max(miss$stage)
flow2$recInt[flow2$miss.flow == 963000]
5108/2880
con<-c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
con<-c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
plot(1:45, con)
plot(con, 1:45, typ='l')
plot(1:45, con)
plot(1:45, con, typ='l')
mod1<-lm(miss$flow[miss$stage < 30] ~ miss$stage[miss$stage < 30])
summary(mod1)
mod2<-lm(miss$flow[miss$stage >= 30] ~ miss$stage[miss$stage >= 30])
summary(mod1)
ANS<-predict(mod2, 30:45)
plot(unique(flow2$miss.flow), unique(flow2$exd), typ="l")
# Download and subset NASS AG Census Data
# Created following https://sheilasaia.rbind.io/post/2019-01-04-nass-api/
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
# commodity description of interest
my_commodity_desc<- "OPERATORS"
# query start year
my_year <- "2000"
# state of interest
my_state <- "ID"
###--------------------------------------#
# Download data and turn into dataframe
#####
# final path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&commodity_desc=", my_commodity_desc, "&year__GE=", my_year, "&state_alpha=", my_state)
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
install.packages(c("httr", "jsonlite", "tidycensus", "tidyverse", "purrr", "mapview", "dplyr"))
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
# commodity description of interest
my_commodity_desc<- "OPERATORS" #FARM OPERATIONS, [AG LAND, INCL BUILDINGS - OPERATIONS WITH ASSET VALUE, MEASURED IN $ / ACRE; $ / OPERATION; $/ACRE; $]; [AG LAND, CROPLAND, PASTURED ONLY - ACRES] [Income, Net or Farm-related?]
# query start year
my_year <- "2000"
# state of interest
my_state <- "ID"
###--------------------------------------#
# Download data and turn into dataframe
#####
# final path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&commodity_desc=", my_commodity_desc, "&year__GE=", my_year, "&state_alpha=", my_state)
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
###--------------------------------------#
# Subset Data
#####
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
All_cat<- unique(id_ops_raw_data$class_desc)
variables<-c("(ALL)", "(ALL), FEMALE")
id_operators <- id_ops_raw_data %>%
#filter to counties in southern Idaho
filter(asd_desc %in% regions) %>%
filter(agg_level_desc == "COUNTY") %>%
filter(class_desc %in% variables)  %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi, county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
head(id_operators)
age_var<-(All_cat[3:9])
ages <- id_ops_raw_data %>%
#filter to counties in southern Idaho
#filter(asd_desc %in% regions) %>%
filter(class_desc %in% age_var)  %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi, county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
library(tidycensus)
install.packages("tidycensus")
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
# commodity description of interest
my_commodity_desc<- "FARM OPERATIONS" #[AG LAND, INCL BUILDINGS - OPERATIONS WITH ASSET VALUE, MEASURED IN $ / ACRE; $ / OPERATION; $/ACRE; $]; [AG LAND, CROPLAND, PASTURED ONLY - ACRES] [Income, Net or Farm-related?]
# query start year
my_year <- "2000"
# state of interest
my_state <- "ID"
###--------------------------------------#
# Download data and turn into dataframe
#####
# final path string
path_id_farms <- paste0("api/api_GET/?key=", nass_key, "&commodity_desc=", my_commodity_desc, "&year__GE=", my_year, "&state_alpha=", my_state)
#unpack JSON object
raw_id_farms <- GET(url = nass_url, path = path_id_farms)
char_raw_id_farms<- rawToChar(raw_id_farms$content)
# check size of object
nchar(char_raw_id_farms)
#turn into list
list_raw_id_farms<- fromJSON(char_raw_id_farms)
# apply rbind to each row of the list and convert to a data frame
id_farms_raw_data <- pmap_dfr(list_raw_id_farms, rbind)
#####--------------------------------------#
# Subset Data to State Level Aggreegates
#####
id_state_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "STATE") %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char)
#####--------------------------------------#
# Subset Data to County Level Aggreegates
#####
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "COUNTY") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
install.packages("stringr", type="source")
library("stringr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
View(list_raw_id_farms)
utils:::menuInstallPkgs(type="source")
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "COUNTY") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
#mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, Value, unit_desc) %>% #value_char =value_trim
# filter out entries with codes '(D)' and '(Z)'
#filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
#mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
#select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "COUNTY") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
#mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, Value, unit_desc) %>% #value_char =value_trim
# filter out entries with codes '(D)' and '(Z)'
#filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
#mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
#select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
#mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
View(id_county_agg)
unique(id_county_agg$domain_desc)
unique(id_county_agg$domaincat_desc)
unique(id_county_agg$class_desc)
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
id_county_agg <- id_farms_raw_data %>%
filter(agg_level_desc == "STATE") %>%
filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
#mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi,county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, Value, unit_desc) %>% #value_char =value_trim
# filter out entries with codes '(D)' and '(Z)'
#filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
#mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
#select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
#mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
View(id_county_agg)
View(id_county_agg)
# Download and subset NASS AG Census Data
# Created following https://sheilasaia.rbind.io/post/2019-01-04-nass-api/
library(httr)
library(jsonlite)
library(tidycensus)
library(tidyverse)
library(purrr)
library(mapview)
library(dplyr)
# If you've never used your tidycensus API key in your R session, run this:
census_api_key("6fd2754dd1bdcc811b51c669667df2873b3bd56e")
nass_key <- "B5240598-2A7D-38EE-BF8D-816A27BEF504" #QuickStats
# NASS url
nass_url <- "http://quickstats.nass.usda.gov"
# commodity description of interest
my_commodity_desc<- "OPERATORS" #FARM OPERATIONS, [AG LAND, INCL BUILDINGS - OPERATIONS WITH ASSET VALUE, MEASURED IN $ / ACRE; $ / OPERATION; $/ACRE; $]; [AG LAND, CROPLAND, PASTURED ONLY - ACRES] [Income, Net or Farm-related?]
# query start year
my_year <- "2000"
# state of interest
my_state <- "ID"
###--------------------------------------#
# Download data and turn into dataframe
#####
# final path string
path_id_ops <- paste0("api/api_GET/?key=", nass_key, "&commodity_desc=", my_commodity_desc, "&year__GE=", my_year, "&state_alpha=", my_state)
#unpack JSON object
raw_id_ops <- GET(url = nass_url, path = path_id_ops)
char_raw_id_ops<- rawToChar(raw_id_ops$content)
# check size of object
nchar(char_raw_id_ops)
#turn into list
list_raw_id_ops<- fromJSON(char_raw_id_ops)
# apply rbind to each row of the list and convert to a data frame
id_ops_raw_data <- pmap_dfr(list_raw_id_ops, rbind)
###--------------------------------------#
# Subset Data to number of operators in each county, and age groups at state level
#####
regions <- c("EAST", "SOUTHWEST", "SOUTH CENTRAL")
All_cat<- unique(id_ops_raw_data$class_desc) #age categories
variables<-c("(ALL)", "(ALL), FEMALE", All_cat[3:9])
id_operators <- id_ops_raw_data %>%
#filter to specific data
#filter(class_desc %in% variables)  %>%
#filter(asd_desc %in% regions) %>%
# trim white space from ends (note: 'Value' is a character here, not a number)
mutate(value_trim = str_trim(Value)) %>%
# select only the columns we'll need
select(state_alpha, state_ansi, county_code, county_name, asd_desc,
agg_level_desc, year, class_desc, domain_desc, domaincat_desc, value_char =value_trim, unit_desc) %>%
# filter out entries with codes '(D)' and '(Z)'
filter(value_char != "(D)" & value_char != "(Z)") %>%
# remove commas from number values and convert to R numeric class
mutate(value = as.numeric(str_remove(value_char, ","))) %>%
# remove unnecessary columns
select(-value_char) %>%
# make a column with the county name and year (we'll need this for plotting)
mutate(county_year = paste0(str_to_lower(county_name), "_", year)) %>%
# make GEOID column to match up with county level spatial data (we'll need this for mapping)
mutate(GEOID = paste0(state_ansi, county_code))
ages<- id_operators %>%
filter( year == 2007) %>%
filter(class_desc %in% All_cat[3:9])%>%
select(-state_ansi, -county_name, -county_code, -asd_desc, -county_year, -GEOID)
All_cat[3:9]
install.packages("data.table")
install.packages("maxnet")
install.packages("xgboost")
library( dplyr )
rm(list=ls())
##### Create a social network of agents
# generate initial population
x <- 1:50
y <- 1:50
# number of iterations
numstep <- 50
numcell <- 2500
pop <- expand.grid(x, y)
View(pop)
names(pop) <- c("x", "y")
library(dataRetrieval)
library(tidyverse)
start=as.Date("1987-10-01")
end= as.Date("2017-09-30")
USGSsites= c("13185000", "13186000", "13190500", "13192200","13200000","13202000")
siteInfo<-readNWISdata(sites= USGSsites, service="site")
### Data Organized by Individual Sites
pCode <- "00060"
Boise_twnSpr <- readNWISuv(siteNumbers = 13185000, parameterCd = pCode, startDate = start, endDate = end) %>% renameNWISColumns() %>% data.frame
SF_featherville <- readNWISuv(siteNumbers = 13186000, parameterCd = pCode, startDate = start, endDate = end) %>% renameNWISColumns() %>% data.frame
Mores<- readNWISuv(siteNumbers=13200000, parameterCd = pCode, startDate = start, endDate = end) %>% renameNWISColumns() %>% data.frame
SF_And<- readNWISuv(siteNumbers = 13190500, parameterCd = pCode, startDate = start, endDate = end) %>% renameNWISColumns() %>% data.frame
SF_Arrow<- readNWISuv(siteNumbers = 13192200, parameterCd = pCode, startDate = start, endDate = end) %>% renameNWISColumns() %>% data.frame
setwd("C:\Users\kek25\Desktop\GitRepos\UrbanStreamflow")
setwd("\Users\kek25\Desktop\GitRepos\UrbanStreamflow")
setwd("Users\kek25\Desktop\GitRepos\UrbanStreamflow")
setwd("~\Users\kek25\Desktop\GitRepos\UrbanStreamflow")
setwd("~/Documents/GitRepos/UrbanStreamflow")
USGS_sites<- read_csv('USGSGages.csv')
siteInfo<-readNWISdata(sites= USGS_sites$Code, service="site")
USGS_sites<- read_csv('USGSGages.csv')
USGS_sites<- read_csv('USGSGages.csv')
View(USGS_sites)
View(USGS_sites)
USGS_sites<- read_csv('USGSGages.csv')
View(USGS_sites)
siteInfo<-readNWISdata(sites= USGS_sites$Code, service="site")
USGS_sites$Code
USGS_sites<- read_csv('USGSGages.csv')
siteInfo<-readNWISdata(sites= USGS_sites$Code, service="site")
library(maps)
library(mapdata)
install.packages("maodata")
install.packages("mapdata")
